{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ac2dc1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c8ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sed_eval loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6516c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 01:23:43.418247: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-08 01:23:43.418680: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-08 01:23:43.464275: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-08 01:23:59.717203: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-08 01:23:59.726236: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-08 01:24:16.757971: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "#%load_ext autoreload\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "#from src.utils.audio_to_spectrograms import *\n",
    "from src.models.yamnet_train import *\n",
    "\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    path_prefix = ''\n",
    "else:\n",
    "    path_prefix = '..'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7771950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sed_eval\n",
    "\n",
    "gt_pkl_path = 'data/processed/yamnet/spectrograms_train_list.pkl'\n",
    "\n",
    "gt_events = pickle.load(open(gt_pkl_path, 'rb'))\n",
    "gt_event_dict = {ref_event['file']: [{'file':ref_event['file'], \n",
    "                    'event_onset':ref_event['onset'], \n",
    "                    'event_offset':ref_event['offset'],\n",
    "                    'event_label':1}]\n",
    "                    for ref_event in gt_events}\n",
    "\n",
    "detection_classes = [1]\n",
    "\n",
    "def calculate_metrics(pred_event_dict, gt_event_dict, time_resolution=1.0, t_collar=0.25):\n",
    "\n",
    "    segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(detection_classes, time_resolution=time_resolution)\n",
    "    for file, estimated_event in pred_event_dict.items():\n",
    "        ref_event = gt_event_dict[file]\n",
    "        segment_based_metrics.evaluate(\n",
    "            reference_event_list=ref_event,\n",
    "            estimated_event_list=estimated_event\n",
    "        )\n",
    "    #print(segment_based_metrics)\n",
    "\n",
    "    return segment_based_metrics.results_overall_metrics()['f_measure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7334cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsolver = Solver(epochs = 5, train_idx=train_idx, path_prefix = path_prefix, device = device, batch_size = 8)\\nsolver.train()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to retrain\n",
    "# manual train test split (stratified)\n",
    "np.random.seed(0)\n",
    "data = pickle.load(open('data/processed/yamnet/spectrograms_train.pkl', 'rb'))\n",
    "train_size = 0.8\n",
    "train_idx = []\n",
    "for label in np.unique(data['event_label']):\n",
    "    choices = np.where(data['event_label'] == label)[0]\n",
    "    train_idx.append(np.sort(np.random.choice(choices, size = int(np.round(len(choices)*train_size)), replace = False)))\n",
    "train_idx = np.sort(np.concatenate(train_idx))\n",
    "\n",
    "\"\"\"\n",
    "solver = Solver(epochs = 5, train_idx=train_idx, path_prefix = path_prefix, device = device, batch_size = 8)\n",
    "solver.train()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340d6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = list(range(len(data[\"event_label\"])))\n",
    "all_files = [f\"{DETECTION_TRAIN_PATH}/train_snipped_scene_{str(i).zfill(4)}.wav\" for i in all_indices]\n",
    "val_idx = [i for i in range(len(data[\"event_label\"])) if i not in train_idx]\n",
    "val_files = [all_files[i] for i in val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696f3ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-08 01:25:52.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.yamnet_train\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1mLoaded model weights from checkpoints/yamnet_detector.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Pre-computing features for 499 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:01<00:00, 398.00it/s]\n",
      "Testing...: 100%|██████████| 499/499 [00:05<00:00, 88.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f_measure': 0.8951048951048951, 'precision': 0.9333333333333333, 'recall': 0.8598848368522073}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#for threshold in np.linspace(0.3, 0.8, 11):\n",
    "solver = Solver(train_idx=[], mode=\"infer\", checkpoint_path = YAMNET_DETECTOR_CHECKPOINT)\n",
    "solver.load_model(YAMNET_DETECTOR_CHECKPOINT)\n",
    "\n",
    "threshold = 0.5\n",
    "print(threshold)\n",
    "events_list = solver.inference(val_files, threshold=threshold, med_filter = False)\n",
    "pred_event_dict = {}\n",
    "for key, value in events_list.items():\n",
    "    for value_i in range(len(value)):\n",
    "        value[value_i] = value[value_i] | {\n",
    "            \"event_label\": 1\n",
    "        }\n",
    "    pred_event_dict[key] = value\n",
    "\n",
    "results = calculate_metrics(pred_event_dict, gt_event_dict, t_collar=0.5)\n",
    "print(results) # 0.8903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f63b8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e34337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-06 11:28:11.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.yamnet_train\u001b[0m:\u001b[36mrun_yamnet\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mLoading from checkpoint checkpoints/yamnet_detector.pth...\u001b[0m\n",
      "\u001b[32m2025-12-06 11:28:12.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.yamnet_train\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mLoaded model weights from checkpoints/yamnet_detector.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing features for 500 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:11<00:00, 43.88it/s]\n",
      "Testing...: 100%|██████████| 500/500 [00:02<00:00, 227.88it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = Path(\"data\") / \"processed\" / \"yamnet\" / \"spectrograms_test.pkl\"\n",
    "test_data = pickle.load(open(test_path, \"rb\"))\n",
    "filepaths = [os.path.join(DETECTION_TEST_PATH, file) for file in test_data['files']]\n",
    "events = run_yamnet(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae53039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'test_snipped_scene_0000.wav',\n",
       "  'event_onset': 1.92,\n",
       "  'event_offset': 5.76}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events['test_snipped_scene_0000.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_pkl_path = 'data/processed/yamnet/spectrograms_test_list.pkl'\n",
    "\n",
    "gt_events = pickle.load(open(gt_pkl_path, 'rb'))\n",
    "gt_event_dict = {ref_event['file']: [{'file':ref_event['file'], \n",
    "                    'event_onset':ref_event['onset'], \n",
    "                    'event_offset':ref_event['offset'],\n",
    "                    'event_label':1}]\n",
    "                    for ref_event in gt_events}\n",
    "\n",
    "pred_event_dict = {}\n",
    "for key, value in events.items():\n",
    "    for value_i in range(len(value)):\n",
    "        value[value_i] = value[value_i] | {\n",
    "            \"event_label\": 1\n",
    "        }\n",
    "    pred_event_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b457f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment based metrics\n",
      "========================================\n",
      "  Evaluated length                  : 4228.63 sec\n",
      "  Evaluated files                   : 500 \n",
      "  Segment length                    : 1.00 sec\n",
      "\n",
      "  Overall metrics (micro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 89.89 %\n",
      "    Precision                       : 92.98 %\n",
      "    Recall                          : 87.00 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 0.20 \n",
      "    Substitution rate               : 0.00 \n",
      "    Deletion rate                   : 0.13 \n",
      "    Insertion rate                  : 0.07 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 87.00 %\n",
      "    Specificity                     : 94.28 %\n",
      "    Balanced accuracy               : 90.64 %\n",
      "    Accuracy                        : 90.89 %\n",
      "\n",
      "  Class-wise average metrics (macro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 89.89 %\n",
      "    Precision                       : 92.98 %\n",
      "    Recall                          : 87.00 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 0.20 \n",
      "    Deletion rate                   : 0.13 \n",
      "    Insertion rate                  : 0.07 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 87.00 %\n",
      "    Specificity                     : 94.28 %\n",
      "    Balanced accuracy               : 90.64 %\n",
      "    Accuracy                        : 90.89 %\n",
      "  \n",
      "\n",
      "  Class-wise metrics\n",
      "  ======================================\n",
      "    Event label  | Nref    Nsys  | F        Pre      Rec    | ER       Del      Ins    | Sens     Spec     Bacc     Acc     \n",
      "    ------------ | -----   ----- | ------   ------   ------ | ------   ------   ------ | ------   ------   ------   ------  \n",
      "    1            | 2085    1951  | 89.9%    93.0%    87.0%  | 0.20     0.13     0.07   | 87.0%    94.3%    90.6%    90.9%   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import sed_eval\n",
    "\n",
    "detection_classes = [1]\n",
    "\n",
    "def calculate_metrics(pred_event_dict, gt_event_dict, time_resolution=1.0, t_collar=0.25):\n",
    "\n",
    "    segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(detection_classes, time_resolution=time_resolution)\n",
    "    for file, estimated_event in pred_event_dict.items():\n",
    "        ref_event = gt_event_dict[file]\n",
    "        segment_based_metrics.evaluate(\n",
    "            reference_event_list=ref_event,\n",
    "            estimated_event_list=estimated_event\n",
    "        )\n",
    "    print(segment_based_metrics)\n",
    "\n",
    "    return segment_based_metrics\n",
    "\n",
    "segment_based_metrics = calculate_metrics(pred_event_dict, gt_event_dict, t_collar=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cee4f2",
   "metadata": {},
   "source": [
    "# Cut audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b3b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main import cut_events_from_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b3ba870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-06 11:28:27.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcut_events_from_audio\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mCreated directory: data/processed/yamnet/extracted_audio\u001b[0m\n",
      "100%|██████████| 500/500 [00:04<00:00, 100.53it/s]\n"
     ]
    }
   ],
   "source": [
    "cut_events_from_audio((Path(\"data\") / \"processed\" / \"yamnet\" / \"extracted_audio\"), events)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
