{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5c6820",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e50456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc08033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease               \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease      \n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 182 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update && sudo apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990a0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
      "Requirement already satisfied: lightning in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.9.1+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.24.1+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.9.1+cu126)\n",
      "Requirement already satisfied: torchlibrosa in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
      "Requirement already satisfied: museval in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
      "Requirement already satisfied: torchcontrib in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
      "Collecting protobuf==5.29.3\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: datasets[audio] in /usr/local/lib/python3.11/dist-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (6.0.3)\n",
      "Requirement already satisfied: torchcodec>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.9.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.15.2)\n",
      "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.8.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: musdb>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from museval) (0.4.3)\n",
      "Requirement already satisfied: simplejson>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from museval) (3.20.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from museval) (4.25.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets[audio]) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets[audio]) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets[audio]) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets[audio]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets[audio]) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets[audio]) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
      "Requirement already satisfied: stempeg>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from musdb>=0.4.0->museval) (0.2.6)\n",
      "Requirement already satisfied: pyaml in /usr/local/lib/python3.11/dist-packages (from musdb>=0.4.0->museval) (25.7.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets[audio]) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets[audio]) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets[audio]) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets[audio]) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets[audio]) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets[audio]) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (2.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->museval) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->museval) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->museval) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->museval) (0.26.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets[audio]) (1.22.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.17.0)\n",
      "Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from stempeg>=0.2.4->musdb>=0.4.0->museval) (0.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets[audio]) (1.3.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets[audio]) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets[audio]) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets[audio]) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets[audio]) (2024.2.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.4->musdb>=0.4.0->museval) (1.0.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets[audio]) (2024.2.0)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.0\n",
      "    Uninstalling protobuf-6.33.0:\n",
      "      Successfully uninstalled protobuf-6.33.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 5.29.3 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-5.29.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U datasets[audio] librosa lightning torch torchvision torchaudio torchlibrosa museval torchcontrib protobuf==5.29.3 tqdm --extra-index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd4f61",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42907bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cd8bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Audio Files: 4564it [00:00, 485512.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "NON_EVENT_LABEL = 'non_event'\n",
    "\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "train_info_df = []\n",
    "\n",
    "for row in tqdm(train_df.itertuples(), desc=\"Loading Audio Files\"):\n",
    "    train_info_df.append(\n",
    "        {\n",
    "            \"file\": row.file_name,\n",
    "            \"file_path\": row.file_path,\n",
    "            \"S_db\": None,\n",
    "            \"onset\": row.onset,\n",
    "            \"offset\": row.offset,\n",
    "            \"event_label\": row.label,\n",
    "            \"background_label\": None,\n",
    "            \"sr\": None\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2c4cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import DETECTION_TRAIN_PATH, DETECTION_TEST_PATH\n",
    "from src.config import CLASSES\n",
    "import pickle\n",
    "\n",
    "NON_EVENT_LABEL = 'non_event'\n",
    "\n",
    "## Train\n",
    "data = pickle.load(open('data/processed/yamnet/spectrograms_train.pkl', 'rb'))\n",
    "num_records = len(data[\"files\"])\n",
    "# train_info_df = []\n",
    "for i in range(num_records):\n",
    "    train_info_df.append(\n",
    "        {\n",
    "            \"file\": data[\"files\"][i],\n",
    "            \"file_path\": f'{DETECTION_TRAIN_PATH}/{data[\"files\"][i]}',\n",
    "            \"S_db\": data[\"S_db\"][i],\n",
    "            \"onset\": data[\"onset\"][i],\n",
    "            \"offset\": data[\"offset\"][i],\n",
    "            \"event_label\": data[\"event_label\"][i],\n",
    "            \"background_label\": data[\"background_label\"][i],\n",
    "            \"sr\": data[\"sr\"]\n",
    "\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "## Test\n",
    "data = pickle.load(open('data/processed/yamnet/spectrograms_test.pkl', 'rb'))\n",
    "num_records = len(data[\"files\"])\n",
    "test_info_df = []\n",
    "for i in range(num_records):\n",
    "    test_info_df.append(\n",
    "        {\n",
    "            \"file\": data[\"files\"][i],\n",
    "            \"file_path\": f'{DETECTION_TEST_PATH}/{data[\"files\"][i]}',\n",
    "            \"S_db\": data[\"S_db\"][i],\n",
    "            \"onset\": data[\"onset\"][i],\n",
    "            \"offset\": data[\"offset\"][i],\n",
    "            \"event_label\": data[\"event_label\"][i],\n",
    "            \"background_label\": data[\"background_label\"][i],\n",
    "            \"sr\": data[\"sr\"]\n",
    "\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce777385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Base from HTS AT Repository\n",
    "from external.hts_audio_transformer import config\n",
    "\n",
    "## Add / Modify Configurations\n",
    "config.debug = True\n",
    "config.max_epoch = 10\n",
    "config.classes_num = len(CLASSES) + 1\n",
    "config.sample_rate = train_info_df[0]['sr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ensure_fixed_waveform_length(waveform: torch.Tensor, target_length: int = 882000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ensures an audio waveform tensor is exactly the target_length by cropping the start \n",
    "    or zero-padding the end, using optimized PyTorch operations.\n",
    "\n",
    "    Args:\n",
    "        waveform (torch.Tensor): The input audio time series tensor (e.g., shape [L]).\n",
    "        target_length (int): The required length for the output waveform.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The processed waveform with the exact target_length.\n",
    "    \"\"\"\n",
    "    original_length = waveform.size(0)\n",
    "\n",
    "    if original_length > target_length:\n",
    "        return waveform[:target_length]\n",
    "        \n",
    "    elif original_length < target_length:\n",
    "        padding_amount = target_length - original_length\n",
    "        padding_tuple = (0, padding_amount)\n",
    "\n",
    "        return F.pad(waveform, padding_tuple, mode='constant', value=0.0)\n",
    "        \n",
    "    else:\n",
    "        return waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c69ca10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8027b00029e40e78abbfa4fd8b23bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n",
      "882000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90/3163772396.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mformatted_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_info_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mformatted_test_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_info_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_90/3163772396.py\u001b[0m in \u001b[0;36mformat_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     25\u001b[0m         }\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     formatted_dataset = dataset.map(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mformat_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m         }\n\u001b[1;32m    561\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3339\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0munprocessed_kwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_kwargs_per_job\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3341\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0munprocessed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3342\u001b[0m                             \u001b[0mcheck_if_shard_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3671\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3672\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3673\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3674\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3675\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3645\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3646\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3647\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3649\u001b[0m         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3568\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_90/3163772396.py\u001b[0m in \u001b[0;36mformat_function\u001b[0;34m(record, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0maudio_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLASS_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Otherwise try soundfile first, and then fall back if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__soundfile_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFileRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# Load the target number of frames, and transpose to match librosa form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sf_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'f_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m         \u001b[0m_error_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errorcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, ClassLabel\n",
    "import librosa\n",
    "\n",
    "CLASS_LABELS = ClassLabel(names=CLASSES + [NON_EVENT_LABEL])\n",
    "\n",
    "train_info_dataset = Dataset.from_list(train_info_df)\n",
    "test_info_dataset = Dataset.from_list(test_info_df)\n",
    "\n",
    "def format_dataset(dataset: Dataset):\n",
    "\n",
    "    def format_function(record, idx):\n",
    "        audio_filename= record[\"file\"]\n",
    "\n",
    "        audio_path = record['file_path']\n",
    "        audio, sr = librosa.load(audio_path, sr=record['sr'])\n",
    "        target = CLASS_LABELS.str2int(record['event_label'])\n",
    "\n",
    "        print(len(audio))\n",
    "\n",
    "        return record | {\n",
    "            \"audio_name\": audio_filename,\n",
    "            \"target\": target,\n",
    "            \"waveform\": audio,\n",
    "            \"real_len\": len(audio)\n",
    "        }\n",
    "\n",
    "    formatted_dataset = dataset.map(\n",
    "        format_function,\n",
    "        with_indices=True,\n",
    "    ).with_format(\"torch\")\n",
    "\n",
    "    return formatted_dataset\n",
    "\n",
    "formatted_train_dataset = format_dataset(train_info_dataset)\n",
    "formatted_test_dataset = format_dataset(test_info_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5428b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 2500\n",
      "Test Dataset: 500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "import random\n",
    "\n",
    "class HTSATDataset(Dataset):\n",
    "    def __init__(self, dataset, config, eval_mode = False):\n",
    "        self.dataset = dataset\n",
    "        self.config = config       \n",
    "        self.total_size = len(self.dataset)\n",
    "        self.queue = [*range(self.total_size)]\n",
    "        logging.info(\"total dataset size: %d\" %(self.total_size))\n",
    "        if not eval_mode:\n",
    "            self.generate_queue()\n",
    "\n",
    "    def generate_queue(self):\n",
    "        random.shuffle(self.queue)\n",
    "        logging.info(\"queue regenerated:%s\" %(self.queue[-5:]))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get actual index of the record\n",
    "        p = self.queue[index]\n",
    "\n",
    "        # Get actual item\n",
    "        record = self.dataset[p]\n",
    "\n",
    "        # one-hot target if required\n",
    "        if self.config.loss_type == 'clip_bce':\n",
    "            label_tensor = torch.as_tensor(record[\"target\"], dtype=torch.long)\n",
    "            target = F.one_hot(label_tensor, num_classes=self.config.classes_num).float()\n",
    "        else:\n",
    "            target = torch.as_tensor(record[\"target\"])\n",
    "\n",
    "        data_dict = record | {\n",
    "            \"target\": target\n",
    "        }\n",
    "        logging.info(f\"getitem: {p}\")\n",
    "        return data_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "\n",
    "htsat_train_dataset = HTSATDataset(formatted_train_dataset, config)\n",
    "htsat_test_dataset = HTSATDataset(formatted_test_dataset, config)\n",
    "\n",
    "print(f\"Train Dataset: {len(htsat_train_dataset)}\")\n",
    "print(f\"Test Dataset: {len(htsat_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a82c5f",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c77ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del minimal_trainer\n",
    "# del sed_model\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a49d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.clip_duration = 20.0\n",
    "config.classes = CLASS_LABELS\n",
    "config.clip_samples = config.sample_rate * config.clip_duration\n",
    "config.htsat_spec_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016e32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch.optim as optim\n",
    "import bisect\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from external.hts_audio_transformer.utils import get_loss_func\n",
    "\n",
    "\n",
    "class HTSATModel(L.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        # self.dataset = dataset\n",
    "        self.loss_func = get_loss_func(config.loss_type)\n",
    "        self.test_step_outputs = []\n",
    "    \n",
    "    def forward(self, x, mix_lambda = None):\n",
    "        output_dict = self.model(x)\n",
    "        return output_dict[\"clipwise_output\"] , output_dict[\"framewise_output\"]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "        mix_lambda = None\n",
    "\n",
    "        batch_waveform = batch['waveform']\n",
    "        pred, pred_map = self(batch_waveform, mix_lambda)\n",
    "\n",
    "        # Get Frame Loss\n",
    "        loss = self.calculate_framewise_loss(batch, pred_map)\n",
    "\n",
    "        # # Get Clip Loss\n",
    "        # target = batch['target']\n",
    "        # clip_loss = self.loss_func(pred, target)\n",
    "\n",
    "        # Combine\n",
    "        # loss = 0.5 * clip_loss + 0.5 * frame_loss\n",
    "\n",
    "        self.log(\"loss\", loss, on_epoch= True, prog_bar=True, batch_size=self.config.batch_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.config.learning_rate, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # Change: SWA, deprecated\n",
    "        # optimizer = SWA(optimizer, swa_start=10, swa_freq=5)\n",
    "        def lr_foo(epoch):       \n",
    "            if epoch < 3:\n",
    "                # warm up lr\n",
    "                lr_scale = self.config.lr_rate[epoch]\n",
    "            else:\n",
    "                # warmup schedule\n",
    "                lr_pos = int(-1 - bisect.bisect_left(self.config.lr_scheduler_epoch, epoch))\n",
    "                if lr_pos < -3:\n",
    "                    lr_scale = max(self.config.lr_rate[0] * (0.98 ** epoch), 0.03 )\n",
    "                else:\n",
    "                    lr_scale = self.config.lr_rate[lr_pos]\n",
    "            return lr_scale\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lr_foo\n",
    "        )\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pred, _ = self(batch[\"waveform\"])\n",
    "        return [pred.detach(), batch[\"target\"].detach()]\n",
    "    \n",
    "    def time_shifting(self, x, shift_len):\n",
    "        shift_len = int(shift_len)\n",
    "        new_sample = torch.cat([x[:, shift_len:], x[:, :shift_len]], axis = 1)\n",
    "        return new_sample \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pred, pred_map = self(batch[\"waveform\"])    # pred: (B, C) | pred_map: (B, window_size, C)\n",
    "        target = batch['target']                    # target: (B, C)\n",
    "        onset = batch['onset']\n",
    "        offset = batch['offset']\n",
    "        loss = self.loss_func(pred, target)\n",
    "\n",
    "        output = {\n",
    "            'file': batch['file'],\n",
    "            'event_label': batch['event_label'],\n",
    "            'pred': pred.detach().cpu(), \n",
    "            'pred_map': pred_map.detach().cpu(),\n",
    "            'target': target.detach().cpu(),\n",
    "            'onset': onset.detach().cpu(),\n",
    "            'offset': offset.detach().cpu(),\n",
    "        }\n",
    "        self.test_step_outputs.append(output)\n",
    "\n",
    "        return\n",
    "    \n",
    "    # def evaluate_metric(self, pred, target):\n",
    "        # # Convert to Numpy for use\n",
    "        # pred_numpy = pred.numpy()\n",
    "        # target_numpy = target.numpy()\n",
    "\n",
    "        # # Accuracy\n",
    "        # predicted_labels = np.argmax(pred_numpy, axis=1)\n",
    "        # true_labels = np.argmax(target_numpy, axis=1)\n",
    "        # acc = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "        # # Loss\n",
    "        # loss = self.loss_func(pred, target).item()\n",
    "\n",
    "        # # mAP\n",
    "        # mAP = average_precision_score(target_numpy, pred_numpy, average='macro')\n",
    "\n",
    "        # return {\n",
    "        #     \"acc\": acc,\n",
    "        #     \"loss\": loss,\n",
    "        #     \"mAP\": mAP\n",
    "        # }\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        test_step_outputs = self.test_step_outputs\n",
    "\n",
    "        # all_preds = torch.cat([x['pred'] for x in test_step_outputs], dim=0)\n",
    "        # all_targets = torch.cat([x['target'] for x in test_step_outputs], dim=0)\n",
    "    \n",
    "        # metric_results = self.evaluate_metric(all_preds, all_targets)\n",
    "\n",
    "        # self.log_dict(metric_results,  on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "    \n",
    "    def calculate_framewise_loss(self, batch, pred_map):\n",
    "        self.device_type = next(self.parameters()).device\n",
    "\n",
    "        batch_size, num_frames, num_classes = pred_map.shape\n",
    "\n",
    "        # Frame-Wise Loss Calculation\n",
    "        # Construct the Frame-wise Target Map\n",
    "        batch_target = batch['target'].to(self.device_type).float() # (shape: 10, 6)\n",
    "        batch_onset = batch['onset'].to(self.device_type).float()  # (shape: 10)\n",
    "        batch_offset = batch['offset'].to(self.device_type).float()   # (shape: 10)\n",
    "\n",
    "        target_map = torch.zeros(batch_size, num_frames, num_classes, device=self.device_type)\n",
    "        target_map[:, :, self.config.classes.str2int(NON_EVENT_LABEL)] = 1\n",
    "\n",
    "        frame_onset = batch_onset / self.config.clip_duration * num_frames   # (shape: 10)\n",
    "        frame_offset = batch_offset / self.config.clip_duration * num_frames   # (shape: 10)\n",
    "\n",
    "        frame_onset = frame_onset.round().long()\n",
    "        frame_offset = frame_offset.round().long()\n",
    "\n",
    "        onset_broadcast = frame_onset.unsqueeze(1)    # Shape (10, 1)\n",
    "        offset_broadcast = frame_offset.unsqueeze(1)  # Shape (10, 1)\n",
    "\n",
    "        frame_indices = torch.arange(num_frames, device=self.device_type)\n",
    "        mask = (frame_indices >= onset_broadcast) & (frame_indices < offset_broadcast)\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(target_map) # (10, 1024, 6)target_expanded = batch_target.unsqueeze(1) # (10, 1, 6)\n",
    "\n",
    "        target_expanded = batch_target.unsqueeze(1) # (10, 1, 6)\n",
    "        target_map = torch.where(mask_expanded, target_expanded, target_map)\n",
    "\n",
    "        loss = self.loss_func(pred_map, target_map)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f94612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = htsat_train_dataset,\n",
    "    num_workers = config.num_workers,\n",
    "    # num_workers = 0,\n",
    "    batch_size = 10,\n",
    "    shuffle = False,\n",
    "    # sampler = train_sampler\n",
    ")\n",
    "\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     dataset = htsat_val_dataset,\n",
    "#     num_workers = config.num_workers,\n",
    "#     # num_workers = 0,\n",
    "#     batch_size = 10,\n",
    "#     shuffle = False,\n",
    "#     # sampler = train_sampler\n",
    "# )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = htsat_test_dataset,\n",
    "    num_workers = config.num_workers,\n",
    "    # num_workers = 0,\n",
    "    batch_size = 10,\n",
    "    shuffle = False,\n",
    "    # sampler = train_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b3fb9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "minimal_trainer = L.Trainer(\n",
    "    # --- Essentials ---\n",
    "    max_epochs=config.max_epoch,         # Stop condition for training\n",
    "    default_root_dir=\"./checkpoints\",     # Where logs and checkpoints go\n",
    "\n",
    "    # --- Minimal Device Configuration (Assuming single GPU/CPU) ---\n",
    "    # Setting devices=1 for modern Lightning (replaces gpus=1)\n",
    "    # If device_num is 0 or 1, you can often just omit this\n",
    "    # to let Lightning detect the device, but specifying is safer.\n",
    "    # Note: If you need to specify CPU/GPU, use: devices=1, accelerator='gpu'/'cpu'\n",
    "    \n",
    "    # Keeping 'gpus' for compatibility with older PL versions in your original code:\n",
    "    # gpus=device_num, \n",
    "    # log_every_n_steps = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b5f6fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "2025-12-02 16:53:29.707652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764694409.888943    5155 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764694409.936774    5155 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ HTSAT_Swin_Transformer │ 28.6 M │ train │     0 │\n",
       "└───┴───────┴────────────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ HTSAT_Swin_Transformer │ 28.6 M │ train │     0 │\n",
       "└───┴───────┴────────────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 27.6 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 1.1 M                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 28.6 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 114                                                                        \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 217                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 27.6 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 1.1 M                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 28.6 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 114                                                                        \n",
       "\u001b[1mModules in train mode\u001b[0m: 217                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9630ba6bca6643e6b4219fde68719395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from external.hts_audio_transformer.model.htsat import HTSAT_Swin_Transformer\n",
    "\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=512,\n",
    "    num_classes=config.classes_num,\n",
    "    config = config\n",
    ")\n",
    "model = HTSATModel(sed_model, config) \n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "minimal_trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "239ef1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e2bc4c8061496eb44d9ec5810b5b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trained_model = model.model\n",
    "# model = HTSATModel(trained_model, config)\n",
    "test_results = minimal_trainer.test(\n",
    "    model, \n",
    "    dataloaders=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def get_events(batch, config):\n",
    "    NON_EVENT_LABEL_INT = CLASS_LABELS.str2int(NON_EVENT_LABEL)\n",
    "\n",
    "    file = batch['file']\n",
    "    pred_map = batch['pred_map']\n",
    "    batch_size, num_frame, num_classes = pred_map.shape\n",
    "\n",
    "\n",
    "    # pred_map_median = median_filter(pred_map, size=(1, 7, 1)) # smoothing between frames\n",
    "    pred_map_gaussian_np = gaussian_filter1d(\n",
    "        input=pred_map, \n",
    "        sigma=3,\n",
    "        axis=1,             # Apply the filter along the time axis (num_frame)\n",
    "        mode='nearest'      # How to handle the boundaries of the frames\n",
    "    )\n",
    "    pred_map = torch.as_tensor(pred_map_gaussian_np)\n",
    "    pred_map_labels = torch.argmax(pred_map, dim=2)\n",
    "\n",
    "    events = {}\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        pred_map_record = pred_map_labels[batch_idx]\n",
    "        file_record = file[batch_idx]\n",
    "        onset_frame = 0\n",
    "        file_events = []\n",
    "        current_label = pred_map_record[0]\n",
    "        for i, pred_label in enumerate(pred_map_record, start = 1):\n",
    "            if pred_label != current_label:\n",
    "                if current_label != NON_EVENT_LABEL_INT:\n",
    "                    onset_time = onset_frame / num_frame * config.clip_duration\n",
    "                    offset_time = i / num_frame * config.clip_duration\n",
    "                    file_events.append(\n",
    "                        {\n",
    "                            'file': file_record,\n",
    "                            'event_onset': onset_time,\n",
    "                            'event_offset': offset_time,\n",
    "                            'event_label': CLASS_LABELS.int2str(current_label.item())\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                onset_frame = i\n",
    "                current_label = pred_label\n",
    "\n",
    "        if current_label != NON_EVENT_LABEL_INT:\n",
    "            onset_time = onset_frame / num_frame * config.clip_duration\n",
    "            offset_time = i / num_frame * config.clip_duration\n",
    "            file_events.append(\n",
    "                {\n",
    "                    'file': file_record,\n",
    "                    'event_onset': onset_time,\n",
    "                    'event_offset': offset_time,\n",
    "                    'event_label': CLASS_LABELS.int2str(current_label.item())\n",
    "                }\n",
    "            )\n",
    "        events[file_record] = file_events\n",
    "    return events\n",
    "\n",
    "pred_event_dict = {}\n",
    "for batch in model.test_step_outputs:\n",
    "    batch_events = get_events(batch, config)\n",
    "    pred_event_dict = pred_event_dict | batch_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb1c3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.utils.audio_to_spectrograms import create_spectrogram_pkl\n",
    "\n",
    "# check if pkl file is created\n",
    "gt_pkl_path = 'data/processed/yamnet/spectrograms_test_list.pkl'\n",
    "if not(os.path.exists(gt_pkl_path)):\n",
    "    create_spectrogram_pkl()\n",
    "\n",
    "gt_events = pickle.load(open(gt_pkl_path, 'rb'))\n",
    "gt_event_dict = {ref_event['file']: [{'file':ref_event['file'], \n",
    "                    'event_onset':ref_event['onset'], \n",
    "                    'event_offset':ref_event['offset'],\n",
    "                    'event_label':ref_event['event_label']}]\n",
    "                    for ref_event in gt_events}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f4eb60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-02 17:41:20.956\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcalculate_metrics\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mMetrics not implemented!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment based metrics\n",
      "========================================\n",
      "  Evaluated length                  : 4130.00 sec\n",
      "  Evaluated files                   : 500 \n",
      "  Segment length                    : 1.00 sec\n",
      "\n",
      "  Overall metrics (micro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 82.27 %\n",
      "    Precision                       : 84.90 %\n",
      "    Recall                          : 79.81 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 0.28 \n",
      "    Substitution rate               : 0.07 \n",
      "    Deletion rate                   : 0.13 \n",
      "    Insertion rate                  : 0.07 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 79.81 %\n",
      "    Specificity                     : 98.51 %\n",
      "    Balanced accuracy               : 89.16 %\n",
      "    Accuracy                        : 96.73 %\n",
      "\n",
      "  Class-wise average metrics (macro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 82.30 %\n",
      "    Precision                       : 85.61 %\n",
      "    Recall                          : 80.34 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 0.34 \n",
      "    Deletion rate                   : 0.20 \n",
      "    Insertion rate                  : 0.15 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 80.34 %\n",
      "    Specificity                     : 98.51 %\n",
      "    Balanced accuracy               : 89.42 %\n",
      "    Accuracy                        : 96.73 %\n",
      "  \n",
      "\n",
      "  Class-wise metrics\n",
      "  ======================================\n",
      "    Event label  | Nref    Nsys  | F        Pre      Rec    | ER       Del      Ins    | Sens     Spec     Bacc     Acc     \n",
      "    ------------ | -----   ----- | ------   ------   ------ | ------   ------   ------ | ------   ------   ------   ------  \n",
      "    cough        | 580     435   | 78.8%    92.0%    69.0%  | 0.37     0.31     0.06   | 69.0%    99.1%    84.0%    95.1%   \n",
      "    dog_bark     | 387     340   | 82.0%    87.6%    77.0%  | 0.34     0.23     0.11   | 77.0%    98.9%    88.0%    97.0%   \n",
      "    gun_shot     | 396     484   | 77.7%    70.7%    86.4%  | 0.49     0.14     0.36   | 86.4%    96.4%    91.4%    95.5%   \n",
      "    siren        | 426     450   | 91.8%    89.3%    94.4%  | 0.17     0.06     0.11   | 94.4%    98.8%    96.6%    98.4%   \n",
      "    car_horn     | 296     251   | 81.2%    88.4%    75.0%  | 0.35     0.25     0.10   | 75.0%    99.3%    87.1%    97.6%   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import sed_eval\n",
    "\n",
    "from src.config import CLASSES\n",
    "\n",
    "def calculate_metrics(pred_event_dict, gt_event_dict, time_resolution=1.0, t_collar=0.25):\n",
    "    logger.warning(\"Metrics not implemented!\")\n",
    "    # Based on event dictionary\n",
    "\n",
    "    # Accuracy\n",
    "\n",
    "    # IoU\n",
    "\n",
    "    # DCASE SED eval: https://tut-arg.github.io/sed_eval/tutorial.html#id1\n",
    "    # event_based_metrics = sed_eval.sound_event.EventBasedMetrics(CLASSES, t_collar=t_collar)\n",
    "    # for file, estimated_event in pred_event_dict.items():\n",
    "    #     ref_event = gt_event_dict[file]\n",
    "    #     event_based_metrics.evaluate(\n",
    "    #         reference_event_list=ref_event,\n",
    "    #         estimated_event_list=estimated_event\n",
    "    #     )\n",
    "    # print(event_based_metrics)\n",
    "\n",
    "    segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(CLASSES, time_resolution=time_resolution)\n",
    "    for file, estimated_event in pred_event_dict.items():\n",
    "        ref_event = gt_event_dict[file]\n",
    "        segment_based_metrics.evaluate(\n",
    "            reference_event_list=ref_event,\n",
    "            estimated_event_list=estimated_event\n",
    "        )\n",
    "    print(segment_based_metrics)\n",
    "\n",
    "    return event_based_metrics, segment_based_metrics\n",
    "\n",
    "event_based_metrics, segment_based_metrics = calculate_metrics(pred_event_dict, gt_event_dict, t_collar=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e2bd885",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a02dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume 'model' is your initialized PyTorch model\n",
    "# Define the path where you want to save the file\n",
    "PATH = 'saved_models/htsat_3Dec_10Epoch.pth'\n",
    "\n",
    "# Use torch.save() to save the state dictionary\n",
    "trained_model = model.model\n",
    "torch.save(trained_model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
