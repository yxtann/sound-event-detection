{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6951c1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! wget https://storage.googleapis.com/audioset/yamnet.h5 -O ../src/models/yamnet.h5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"! wget https://storage.googleapis.com/audioset/yamnet.h5 -O ../src/models/yamnet.h5\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f841e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f1946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nictann\\Desktop\\temp_env\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nictann\\Desktop\\temp_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "finetune_yamnet_tf.py\n",
    "Full TensorFlow pipeline to fine-tune YAMNet for a small custom label set.\n",
    "Produces a clip-level classifier on top of YAMNet embeddings, plus sliding-window\n",
    "inference helper to approximate onset/offset.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "#import random\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "#from tensorflow import keras\n",
    "#import librosa\n",
    "#import soundfile as sf\n",
    "#from pathlib import Path\n",
    "import sys\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    path_prefix = ''\n",
    "else:\n",
    "    path_prefix = '..'\n",
    "\n",
    "split = 'train'\n",
    "file_path = os.path.join(path_prefix, 'data/detection', split)\n",
    "\n",
    "UNFREEZE_YAMNET = True      # set True to fine-tune YamNet weights (careful: small LR)\n",
    "TARGET_SR = 16000            # YAMNet requires 16kHz       \n",
    "RANDOM_SEED = 0\n",
    "\n",
    "# For sliding-window inference (higher temporal resolution)\n",
    "SLIDING_WIN_SEC = 1.0   # window length for inference\n",
    "SLIDING_HOP_SEC = 0.1   # hop between windows -> effective temporal resolution\n",
    "\n",
    "%autoreload 2\n",
    "from src.single_stage_yamnet_finetune import *\n",
    "\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5ef412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sr', 'S_db', 'files', 'onset', 'offset', 'event_label', 'background_label'])\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(os.path.join(path_prefix, f'data/processed/detection_{split}.p') , 'rb'))\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcb1961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual train test split (stratified)\n",
    "np.random.seed(0)\n",
    "train_size = 0.8\n",
    "train_idx = []\n",
    "for label in np.unique(data['event_label']):\n",
    "    choices = np.where(data['event_label'] == label)[0]\n",
    "    train_idx.append(np.sort(np.random.choice(choices, size = int(np.round(len(choices)*train_size)), replace = False)))\n",
    "train_idx = np.sort(np.concatenate(train_idx))\n",
    "val_idx = [i for i in range(len(data['event_label'])) if i not in train_idx]\n",
    "len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4e3ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['car_horn' 'cough' 'dog_bark' 'gun_shot' 'siren']\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(data['event_label'])\n",
    "NUM_CLASSES = len(classes)\n",
    "class_to_id = {c:i for i,c in enumerate(classes)}\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "filepaths = [os.path.join(file_path, file) for file in data['files']]\n",
    "labels = [class_to_id[c] for c in data['event_label']]\n",
    "\n",
    "train_files = [filepaths[i] for i in train_idx]\n",
    "train_labels = [labels[i] for i in train_idx]\n",
    "val_files = [filepaths[i] for i in val_idx]\n",
    "val_labels = [labels[i] for i in val_idx]\n",
    "val_onsets = [data['onset'][i] for i in val_idx]\n",
    "val_offsets = [data['offset'][i] for i in val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad4e841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ yam_net_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">YamNetLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ yam_net_layer_1 (\u001b[38;5;33mYamNetLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m1,285\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,685</span> (1.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m263,685\u001b[0m (1.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,685</span> (1.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,685\u001b[0m (1.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/30\n",
      "     25/Unknown \u001b[1m31s\u001b[0m 505ms/step - accuracy: 0.2554 - loss: 1.6006"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nictann\\Desktop\\temp_env\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 733ms/step - accuracy: 0.2375 - loss: 1.5972 - val_accuracy: 0.4100 - val_loss: 1.5564 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 691ms/step - accuracy: 0.3350 - loss: 1.5375 - val_accuracy: 0.5400 - val_loss: 1.4959 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 585ms/step - accuracy: 0.4550 - loss: 1.4933 - val_accuracy: 0.5900 - val_loss: 1.4434 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 614ms/step - accuracy: 0.5050 - loss: 1.4452 - val_accuracy: 0.6300 - val_loss: 1.3910 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 712ms/step - accuracy: 0.5175 - loss: 1.3986 - val_accuracy: 0.6300 - val_loss: 1.3372 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 654ms/step - accuracy: 0.5875 - loss: 1.3462 - val_accuracy: 0.6600 - val_loss: 1.2846 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 582ms/step - accuracy: 0.6275 - loss: 1.2983 - val_accuracy: 0.6800 - val_loss: 1.2332 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 581ms/step - accuracy: 0.6250 - loss: 1.2596 - val_accuracy: 0.6800 - val_loss: 1.1839 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 540ms/step - accuracy: 0.6300 - loss: 1.2214 - val_accuracy: 0.7100 - val_loss: 1.1351 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 511ms/step - accuracy: 0.6875 - loss: 1.1717 - val_accuracy: 0.7600 - val_loss: 1.0952 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 511ms/step - accuracy: 0.6675 - loss: 1.1325 - val_accuracy: 0.8000 - val_loss: 1.0519 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 533ms/step - accuracy: 0.6625 - loss: 1.1106 - val_accuracy: 0.8000 - val_loss: 1.0149 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 494ms/step - accuracy: 0.6950 - loss: 1.0616 - val_accuracy: 0.8300 - val_loss: 0.9832 - learning_rate: 1.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 492ms/step - accuracy: 0.7150 - loss: 1.0344 - val_accuracy: 0.8300 - val_loss: 0.9507 - learning_rate: 1.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 466ms/step - accuracy: 0.7250 - loss: 0.9943 - val_accuracy: 0.8200 - val_loss: 0.9194 - learning_rate: 1.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 493ms/step - accuracy: 0.7350 - loss: 0.9715 - val_accuracy: 0.8300 - val_loss: 0.8926 - learning_rate: 1.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 506ms/step - accuracy: 0.7350 - loss: 0.9463 - val_accuracy: 0.8300 - val_loss: 0.8687 - learning_rate: 1.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 567ms/step - accuracy: 0.7500 - loss: 0.9074 - val_accuracy: 0.8300 - val_loss: 0.8470 - learning_rate: 1.0000e-04\n",
      "Saved model to ../src/models/yamnet_finetune_model.keras\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(train_files, train_labels, val_files, val_labels, lr = 1e-4, epochs = 30, batch_size = 16, device = 'cpu', model_save_dir = \"../src/models/yamnet_finetune_model.keras\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5eca4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label: gun_shot\n",
      "Actual onset: 7.326152328956148\n",
      "Actual offset: 10.0\n",
      "Clip-level prediction: gun_shot (0.284)\n",
      "Sliding-window results (time centers):\n",
      "  car_horn: 3.60s -> 6.60s (max prob 0.944)\n",
      "  dog_bark: 0.00s -> 1.10s (max prob 0.324)\n",
      "  dog_bark: 0.30s -> 1.90s (max prob 0.388)\n",
      "  dog_bark: 1.10s -> 3.20s (max prob 0.385)\n",
      "  dog_bark: 2.40s -> 3.40s (max prob 0.326)\n",
      "  dog_bark: 5.80s -> 6.90s (max prob 0.313)\n",
      "  dog_bark: 6.10s -> 8.10s (max prob 0.341)\n",
      "  gun_shot: 7.50s -> 10.00s (max prob 0.986)\n",
      "  siren: 0.80s -> 1.80s (max prob 0.305)\n",
      "  siren: 1.00s -> 2.00s (max prob 0.335)\n",
      "  siren: 2.80s -> 3.80s (max prob 0.346)\n",
      "  siren: 3.10s -> 4.10s (max prob 0.361)\n",
      "  siren: 3.50s -> 4.50s (max prob 0.331)\n",
      "  siren: 5.00s -> 6.20s (max prob 0.458)\n",
      "  siren: 5.50s -> 6.50s (max prob 0.333)\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "test_path = val_files[i]\n",
    "print(f'Actual label: {classes[val_labels[i]]}')\n",
    "print(f'Actual onset: {val_onsets[i]}')\n",
    "print(f'Actual offset: {val_offsets[i]}')\n",
    "wav = load_audio_mono(test_path, sr=TARGET_SR)\n",
    "clip_idx, clip_prob, clip_vec = trainer.predict_clip(wav)\n",
    "print(f\"Clip-level prediction: {classes[clip_idx]} ({clip_prob:.3f})\")\n",
    "\n",
    "times, probs = trainer.sliding_window_inference(wav, win_sec=SLIDING_WIN_SEC, hop_sec=SLIDING_HOP_SEC)\n",
    "# For each class, convert sliding-window probs into onset/offset segments\n",
    "THRESH = 0.3\n",
    "print(\"Sliding-window results (time centers):\")\n",
    "for cid, cname in enumerate(classes):\n",
    "    mask = probs[:, cid] >= THRESH\n",
    "    if not np.any(mask):\n",
    "        continue\n",
    "    # Merge consecutive windows into segments\n",
    "    idxs = np.where(mask)[0]\n",
    "    splits = np.split(idxs, np.where(np.diff(idxs) != 1)[0] + 1)\n",
    "    for seg in splits:\n",
    "        start_time = times[seg[0]] - SLIDING_WIN_SEC / 2.0\n",
    "        end_time   = times[seg[-1]] + SLIDING_WIN_SEC / 2.0\n",
    "        start_time = max(0.0, start_time)\n",
    "        end_time = min(len(wav)/TARGET_SR, end_time)\n",
    "        print(f\"  {cname}: {start_time:.2f}s -> {end_time:.2f}s (max prob {float(np.max(probs[seg, cid])):.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
